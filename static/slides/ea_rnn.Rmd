---
title: "Evolving Recurrent Neural Network Topologies"
author: "Christian Hower"
date: "Fall, 2018"
output: 
  xaringan::moon_reader:
    css: "robot-fonts"
---

# **Evolving RNN's for Time Series Prediction** 
### <span style="color:blue">Model Tuning</span>
* Large number configurations given class
* Hyperparameter choices important for performance
* Evolutionary Algorithms can automate the search
* 2 Learning Problems, 2 Levels
* Individual level - Prediction problem
* Population level - Search Problem 

---

# **Prediction Problem**
### <span style="color:blue">Data</span>
* Simulated time series data
* Linear combination of 3 sin waves at 3 different scales
* Random Uniform Noise

---

![](.\images\ea_rnn_simulated_data.jpg)

---

# **Prediction Problem** 
### <span style="color:blue">Model</span>
* LSTM
* 3 layers
* input layer > hidden layer > output layer 
* Number of input nodes and hidden nodes learned by EA process
* Error measured in RMSE

---
![](.\images\ea_rnn_training_loss.jpg)


---
# **Search Problem** 
### <span style="color:blue">Population Search</span>
* Model hyperparameters can be represented as a space
* Many good solutions exist in space
* Want to find best combination of simple and accurate
i.e. find Fittest individual in population space

---
# **Search Problem** 
### <span style="color:blue">Evolutionary Algorithm</span>
* Fitness - based on model performance + penalty
* Selection - based on fitness
* Mating - Traits crossover with uniform probability
* Mutation - individuals selcted unformly for mutation
++ bits in genome mutate with uniform probability
++ traits with longer bit encodings mutate with higher prob
* Evaluation - Train model and assign fitness based on model performance

---
# **Search Problem**
### <span style="color:blue">Fitness</span>
* Fitness function is a critical part of EA
* RMSE
* RMSE + Complexity Penalty

---
# **Search Problem**
### <span style="color:blue">Complexity Penalty</span>
* Alpha is maximum penalty
* Proportion of total nodes/nodes possible
* add to rmse 
* interaction between traits and environment

---
# **Complexity Penalty Experiment 1**
### <span style="color:blue">Overview</span>
* 6 generations
* 5 individuals per generation
* 30 total individual
* 2048 possible in model space
* 6% of model space explored per run
* 20 training epochs

---
# **Complexity Penalty Experiment 1**  
### <span style="color:blue">Varying alpha</span>
* Encourages poulation search in different regions 
* Model discovers statefulness property of LSTM?
* Population == sublattice


---
![](.\images\ea_rnn_modelspace_alpha_search.jpg)

---


# **Complexity Penalty Experiment 2** 
### <span style="color:blue">Searching in a larger space</span>
* Add activation function at the output node to GA search
* 8192 states
* 10 generations
* 8 individuals per generation
* 80 total individual
* .01 > modelspace explored 

---
# **Complexity Penalty Experiment 2** 
### <span style="color:blue">Results</span>
* dropout selection?
* LSTM statefulness property discovered?
* train best models with more training epochs

---
![](.\images\ea_rnn_large_search.jpg)



